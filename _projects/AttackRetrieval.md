---
layout: page
title: Corpus Poisoning Attacks for Dense Retrievers
# pdf: /assets/pdf/.pdf
description: Collabrating with Prof. <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>, <a href="https://www.cs.princeton.edu/~zzhong/">Zexuan Zhong</a>, and <a href="https://www.cs.princeton.edu/~awettig/"> Alexander Wettig</a>. Submitted to ACL 2023.
importance: 1
category: research
---

Adversarial methods, including evaluation and training, have been widely used in computer vision as well as some NLP tasks, such as classification. However, whether and how we can apply such methods to retrieval tasks are still under exploited. In this work, we apply adversarial attacks on dense retrievers, so as to build a strong benchmark that can evaluate the robustness of state-of-the-art models on both in-domain and out-of-domain datasets. Our results show that with proper constraints, the attack works well on dense models while keeping the BM25 performance on the attack datasets almost unchanged, showing that the lexical change is not large. By doing so, we give some insights into different types of retrieval models and reveal that the current retrievers still have room for improvement.
